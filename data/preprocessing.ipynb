{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "raw_folder = './raw_data/'\n",
    "tmp_folder = './tmp_data/'\n",
    "processed_folder = './processed_data/'\n",
    "\n",
    "imdb_rating_folder = raw_folder + 'imdb/'\n",
    "cmu_folder = raw_folder + 'cmu/'\n",
    "baby_names_folder = raw_folder + 'baby_names_national/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "In this notebook we aim to select the relevant features for our model and to clean the data. This also includes merging content from the different datasets, from various sources such as the `IMDB` datasets, the baby names datasets, etc... which have been either downloaded or scraped from the web.\n",
    "\n",
    "**Note on structure:** We separate dataframes into various folders, raw, tmp (temporary data) and processed. \n",
    "- The raw data is what we download from the web, including the initial CMU datasets. These are unmodified content that we use as a basis for our work, it is read-only.\n",
    "- The temporary (tmp) data is the data that we use to work on, and that we modify. This is often the case for scrapped data from the web, or dataframes that are being used in a different notebook other than processing.\n",
    "- We put our final cleaned data in the processed folder, which is the data that we use for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial view on the data\n",
    "### a. CMU datasets\n",
    "Our project requires the use of the `character.metadata.tsv` and `movie.metadata.tsv` files from the CMU databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import character metadata\n",
    "character_df = pd.read_csv(cmu_folder + 'character.metadata.tsv', sep='\\t', header=None)\n",
    "\n",
    "# Add column names deduced from README\n",
    "character_df.columns = ['wiki_ID', 'free_ID', 'release', 'char_name', 'DOB', 'gender', 'height', 'ethnicity', 'act_name', 'age_at_release', 'free_char_map1', 'free_char_map2', 'free_char_map3']\n",
    "\n",
    "display(character_df.head())\n",
    "print(f\"There are {len(character_df)} characters in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import movie metadata\n",
    "movie_df = pd.read_csv(cmu_folder + 'movie.metadata.tsv', sep='\\t', header=None)\n",
    "\n",
    "# Add column names deduced from README\n",
    "movie_df.columns = ['wiki_ID', 'free_ID', 'mov_name', 'release', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    "display(movie_df.head(5))\n",
    "print(f\"There are {len(movie_df)} movies in the dataset.\")\n",
    "print(f\"Is the wiki_ID unique? {movie_df.wiki_ID.is_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can use the wikipedia IDs as unique identifers for the movies!\n",
    "\n",
    "We immediately see that there are some columns that we won't need in our analysis, which we will drop later on, such as the freebase character maps, releases in the characters dataframe (because they are already in the movies dataframe), etc...\n",
    "\n",
    "We also see that there are some missing values for `revenues`, we need to take note of how any of them are missing to see if they would affect our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_revenue_missing = movie_df['revenue'].isna().sum()\n",
    "total_movies = len(movie_df)\n",
    "perc_missing = (nb_revenue_missing / total_movies)*100\n",
    "print(f\"Percentage of missing values in column 'revenue': {perc_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. IMDB datasets: Ratings\n",
    "We would like to use ratings and rating counts to assess the popularity of a movie. We will use the `imdb_ratings.tsv` file from the [IMDB datasets](https://datasets.imdbws.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imdb data\n",
    "rating_df = pd.read_csv(imdb_rating_folder + 'imdb_rating.tsv', sep='\\t', index_col='tconst')\n",
    "display(rating_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the indexes are unique\n",
    "print(f\"Is the indexing unique ? {rating_df.index.is_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tconst` is a unique identifer in the IMDB database, which we can use to merge with our other dataframes. We need to figure out how to merge these dataframes together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Baby names data\n",
    "We consider the names of babies across the years in the USA to be a good indicator of the popularity of a name. We will use the `NationalNames.csv` file from the [US Social Security Administration](https://www.ssa.gov/oact/babynames/limits.html), and will be put in the `baby_names_national` folder. This dataset has all the names of babies born in the USA from 1880 to 2023, where each count is the number of babies born with that name in that year.\n",
    "\n",
    "We need to iterate over all the years to create a dataframe with the counts of all the names, for all years:\n",
    "- `name`: name of the baby\n",
    "- `year`: year of birth\n",
    "- `number`: number of babies born with that name in that year \n",
    "- `percentage`: percentage of babies born with that name in that year, for normalization purposes (added later in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset folder\n",
    "folder_path = 'raw_data/baby_names_national/'\n",
    "\n",
    "# Create an empty list to store individual DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.startswith('yob') and filename.endswith('.txt'):\n",
    "        # Extract the year from the filename\n",
    "        year = int(filename[3:-4])\n",
    "\n",
    "        # Read the data from the file into a DataFrame\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path, header=None, names=['name', 'gender', 'number'])\n",
    "\n",
    "        # Add the 'year' column to the DataFrame\n",
    "        df['year'] = year\n",
    "\n",
    "        # Append the current DataFrame to the list\n",
    "        data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one DataFrame\n",
    "baby_name_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Set the 'name' column as the index\n",
    "# combined_data.set_index('name', inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "baby_name_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing and cleaning\n",
    "In this section we modify some of the dataframes values to make them usable for our analysis, and populate them with extra data from other sources.\n",
    "\n",
    "### a. Linking external movie IDs (Scraping part 1)\n",
    "One of our main sources of data is the [IMDB](https://www.imdb.com/) and the [TMDB](https://www.themoviedb.org/) website. To be able to do so, we need to find a link from our CMU IDs (wikipedia) to the IMDB and TMDB movie IDs. We've explored 2 methods of collecting this kind of data. All the process is extensively documented in the `convert_ds.ipynb` notebook. \n",
    "\n",
    "Therefore make sure you run [this notebook](./scraping/convert_ids.ipynb) before continuing. It should have generated the `movies_external_ids.csv` file in the `tmp` folder, which we will use in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test import with a try\n",
    "try:\n",
    "    df = pd.read_csv(tmp_folder + 'movies_external_ids.csv', sep='\\t', header=None)\n",
    "except:\n",
    "    raise FileNotFoundError(\"Please run the ID scrapping notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Movie metadata preprocessing\n",
    "#### Release date column\n",
    "The release date column is very important for our study. Let's compute the missing values and remove the NaN value in the release column, we will drop these rows later on.\n",
    "\n",
    "# <h1 style=\"font-size: 50px; color: red\">⚠️**TODO:** change structure instead of `release` to `year` and `month` columns. If no month, just leave nan!</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_release_missing = movie_df['release'].isna().sum()\n",
    "total_movies = len(movie_df)\n",
    "perc_missing = (nb_release_missing / total_movies)*100\n",
    "print(f\"Percentage of missing values in column 'release': {perc_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". The movies release has a weird format. There are 3 different formats:\n",
    "- of length 10: `YYYY-MM-DD`\n",
    "- of length 7: `YYYY-MM`\n",
    "- of length 4: `YYYY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_lengths = movie_df['release'].dropna().apply(lambda x: len(str(x))).value_counts()\n",
    "print(f\"In the release column of movie_df the string have the following lengths with their frequency : \\n\\n{counts_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length4_test = movie_df[movie_df['release'].apply(lambda x: len(str(x)) == 4)].iloc[0]['release']\n",
    "print(f\"Example of a value of length 4: {length4_test}\")\n",
    "length7_test = movie_df[movie_df['release'].apply(lambda x: len(str(x)) == 7)].iloc[0]['release']\n",
    "print(f\"Example of a value of length 7: {length7_test}\")\n",
    "length10_test = movie_df[movie_df['release'].apply(lambda x: len(str(x)) == 10)].iloc[0]['release']\n",
    "print(f\"Example of a value of length 10: {length10_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should convert the release date to keep only the year as we have only the year in the baby name dataset. To do so we simply take the first 4 characters of the release date, which is the year. We then convert it to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the release column\n",
    "movie_datetime_df = movie_df.dropna(subset=['release']).copy(deep=True)\n",
    "\n",
    "# Take only year\n",
    "movie_datetime_df['release'] = movie_datetime_df['release'].apply(lambda x: str(x)[:4]).astype(np.int64)\n",
    "\n",
    "# Remove outliers (years before 1800, there is one that is at 1010)\n",
    "movie_datetime_df = movie_datetime_df[movie_datetime_df['release'] >= 1800]\n",
    "\n",
    "movie_datetime_df[['wiki_ID', 'mov_name', 'release']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the movie_df to the datetime version\n",
    "movie_df = movie_datetime_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add movie ratings and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import idmb with wikidata index mapping\n",
    "movies_to_imdb_id_df = pd.read_csv(tmp_folder + 'movies_external_ids.csv')\n",
    "display(movies_to_imdb_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have the rating of all the movies. To do so we carry out a left merge and count the number of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_merged_imdb_movie_df = pd.merge(movie_df.reset_index(), movies_to_imdb_id_df, left_on='wiki_ID', right_on='wikipedia_ID', how='left').copy(deep=True)\n",
    "display(left_merged_imdb_movie_df.head(2))\n",
    "print(f\"Length of the dataframe : {len(left_merged_imdb_movie_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rating_missing = left_merged_imdb_movie_df['IMDB_ID'].isna().sum()\n",
    "total_movies = len(left_merged_imdb_movie_df)\n",
    "perc_missing = (nb_rating_missing / total_movies)*100\n",
    "print(f\"Percentage of missing values in column 'averageRating': {perc_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We miss about 5% of the IMDB index. Therefore, the number of movie is reduced of 5% as well. Let's make the inner merge to keep only the movie for which we know the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_imdb_movie_df = pd.merge(movie_df.reset_index(), movies_to_imdb_id_df, left_on='wiki_ID', right_on='wikipedia_ID', how='inner').copy(deep=True)\n",
    "display(merged_imdb_movie_df.head(2))\n",
    "print(f\"Length of the dataframe : {len(merged_imdb_movie_df)}\")\n",
    "\n",
    "merged_rating_movie_df = pd.merge(merged_imdb_movie_df, rating_df, left_on='IMDB_ID', right_on='tconst', how='inner').copy(deep=True)\n",
    "display(merged_rating_movie_df.head(2))\n",
    "print(f\"Length of the dataframe : {len(merged_rating_movie_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remove the columns that we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_df = merged_rating_movie_df[['wiki_ID', 'mov_name', 'release', 'revenue', 'genres', 'numVotes', 'averageRating']].copy(deep=True)\n",
    "display(movie_rating_df.head(2))\n",
    "print(f\"length of the dataframe : {len(movie_rating_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_rating_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre column cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Are there movies with missing genre fields? {movie_df['genres'].isna().sum() != 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the genre column we have dictionaries. The key is a freebase value, and the value is the associated string. So we'll keep the string only, and we'll end up with a simple list of genres.\n",
    "\n",
    "We want to generate an aditional dataset that has the wikiID of a movie and the genre asosciated with that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_convert(genres_raw):\n",
    "    \"\"\"Converts the genres column from a string to a list of genres\n",
    "    \"\"\"\n",
    "    genres_list = []\n",
    "    genres_raw = json.loads(genres_raw)\n",
    "\n",
    "    # Keep only values\n",
    "    for genre in genres_raw.values():\n",
    "        genres_list.append(genre)\n",
    "\n",
    "    return genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform genres column from json string to list\n",
    "movies_processed_genres = movie_df.copy(deep=True)\n",
    "movies_processed_genres['genres'] = movies_processed_genres['genres'].apply(genres_convert)\n",
    "\n",
    "# Create a new DataFrame with each genre in a separate row\n",
    "expanded_genres = pd.DataFrame(columns=['wiki_ID', 'genre'])\n",
    "\n",
    "# We don't directly add rows to the DataFrame because it is inefficient, so we create lists to store the values\n",
    "exp_wiki_ids = []\n",
    "exp_genres = []\n",
    "for index, row in movies_processed_genres.iterrows():\n",
    "    # Extract wikiID and genres dictionary\n",
    "    wiki_id = row['wiki_ID']\n",
    "    genre_list = row['genres']\n",
    "\n",
    "    # Iterate through each genre in the genres dictionary\n",
    "    for genre in genre_list:\n",
    "        # Append each genre as a new row to the expanded_genres DataFrame\n",
    "        exp_wiki_ids += [wiki_id]\n",
    "        exp_genres += [genre]\n",
    "\n",
    "expanded_genres['wiki_ID'] = exp_wiki_ids\n",
    "expanded_genres['genre'] = exp_genres\n",
    "\n",
    "# Set index to wikiID\n",
    "expanded_genres.set_index('wiki_ID', inplace=True)\n",
    "expanded_genres.sort_index(inplace=True)\n",
    "expanded_genres.groupby('genre').count().sort_values(by='genre', ascending=False)\n",
    "\n",
    "print(f\"Number of different genres: {len(expanded_genres['genre'].unique())}\")\n",
    "\n",
    "display(expanded_genres.head())\n",
    "print(f\"Length of the genres dataframe : {len(expanded_genres)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataframe\n",
    "expanded_genres.to_csv(processed_folder + 'movie_genres_df.csv')\n",
    "\n",
    "# Remove the genres column from the movie_df\n",
    "movie_df.drop(columns=['genres'], inplace=True)\n",
    "display(movie_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a simple data exploration on the genres of the movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of movies per genre\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.countplot(y='genre', data=expanded_genres, order=expanded_genres['genre'].value_counts().index[:40]) # only take top 40 genres\n",
    "\n",
    "plt.title('Number of Movies per Genre')\n",
    "plt.xlabel('Number of Movies')\n",
    "plt.ylabel('Genre')\n",
    "plt.show()\n",
    "\n",
    "# Count the number of genres in movies\n",
    "genre_counts = expanded_genres.groupby('wiki_ID')['genre'].count()\n",
    "genre_count_frequency = genre_counts.value_counts().sort_index()\n",
    "\n",
    "genre_count_frequency.plot(kind='bar')\n",
    "plt.title('Number of Genres per Movie')\n",
    "plt.xlabel('Number of Genres')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Baby name data preprocessing\n",
    "First we combine the rows with the same name and same year in order to ignore the gender in the dataset: we study the overall impact of the name regardless of the gender of the baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_name_filtered_df = baby_name_df.groupby(['name', 'year'])['number'].sum().to_frame()\n",
    "display(baby_name_filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each datapoint, compute the percentage of the total births in the year. We do this to normalize the data, in case the number of births per year has increased over the years or if a year had notably more births than another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the baby name dataframe by year to get the number of birth per year.\n",
    "birth_per_year_df = baby_name_filtered_df.groupby('year')['number'].sum().to_frame()\n",
    "birth_per_year_df.reset_index(inplace=True)\n",
    "\n",
    "birth_per_year_df = birth_per_year_df.rename(columns={'number': 'total_number'})\n",
    "\n",
    "print(\"birth_per_year_df:\")\n",
    "display(birth_per_year_df.head(3))\n",
    "\n",
    "# Merge dataframes\n",
    "merged_df = pd.merge(baby_name_filtered_df.reset_index(), birth_per_year_df, on='year')\n",
    "\n",
    "# Calculate the percentage and add it as a new column to dataframe\n",
    "merged_df['percentage'] = (merged_df['number'] / merged_df['total_number']) * 100\n",
    "\n",
    "print(\"merged_df:\")\n",
    "display(merged_df.head(3))\n",
    "\n",
    "baby_name_with_percentage_df = merged_df.drop('total_number', axis=1)\n",
    "baby_name_with_percentage_df.set_index(['name', 'year'], inplace=True)\n",
    "\n",
    "print(\"baby_name_with_percentage_df:\")\n",
    "display(baby_name_with_percentage_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Character metadata preprocessing\n",
    "The character dataframe has not a unique index. It is due to the several NaN values present in the same movie. To tackles this we can drop the rows that have a NaN as name and see if it solve the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_filtered = character_df.copy(deep=True)\n",
    "\n",
    "# Drop the rows with a NaN as the character name\n",
    "character_filtered = character_filtered.dropna(subset=['char_name'])\n",
    "\n",
    "# Drop the rows with the same character name\n",
    "character_filtered = character_filtered.drop_duplicates(subset=['wiki_ID','char_name','gender'])\n",
    "character_filtered = character_filtered.set_index(['wiki_ID','char_name', 'gender'])\n",
    "\n",
    "display(character_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop all columns that are not interesting for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['free_ID', 'release', 'ethnicity', 'free_char_map1', 'free_char_map2', 'free_char_map3']\n",
    "columns_to_drop_existing = [col for col in columns_to_drop if col in character_filtered.columns]\n",
    "\n",
    "character_filtered.drop(columns_to_drop_existing, axis=1, inplace=True)\n",
    "\n",
    "display(character_filtered.head())\n",
    "\n",
    "print(f\"Length of the dataframe: {len(character_filtered)}\")\n",
    "\n",
    "# Verify the indexes are unique\n",
    "print(f\"Is the indexing unique ? {character_filtered.index.is_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize, we remove all the characters that are not in the movies dataframe (since we cleaned that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all the movie IDs\n",
    "movie_ids = movie_df['wiki_ID'].copy(deep=True)\n",
    "\n",
    "# Merge the two dataframes\n",
    "dump_characters = character_filtered.reset_index().set_index('wiki_ID')\n",
    "character_filtered = dump_characters.merge(movie_ids, on='wiki_ID', how='inner')\n",
    "\n",
    "# Reset the index to before\n",
    "character_filtered = character_filtered.set_index(['wiki_ID', 'char_name', 'gender'])\n",
    "\n",
    "print(f\"Length of the dataframe: {len(character_filtered)}\")\n",
    "print(f\"Is the indexing unique ? {character_filtered.index.is_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a dataframe with the name of the characetrs and the wikiID of the movie, and their genders, called `name_by_movie_df.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = character_filtered.reset_index()[['wiki_ID', 'char_name', 'gender']].copy(deep=True)\n",
    "display(name_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the character names and explode it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the character names into words and explode the lists\n",
    "exploded_name_df = name_df.assign(char_words=name_df['char_name'].str.split()).explode('char_words')\n",
    "word_name_df = exploded_name_df.drop(columns=['char_name'])\n",
    "display(word_name_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take care of duplicates for both the characters dataframe (repeated names) and the baby names (keep only one per year), so that we can merge them without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with the same character name\n",
    "print(f\"Size before duplicates removal: {len(word_name_df)}\")\n",
    "word_name_df = word_name_df.drop_duplicates(subset=['wiki_ID', 'gender', 'char_words'])\n",
    "print(f\"Size after duplicates removal: {len(word_name_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_name_only_df = baby_name_df[['name']].copy(deep=True)\n",
    "\n",
    "# Does the baby_name_only_df has duplicates?\n",
    "duplicates = baby_name_only_df.duplicated()\n",
    "print(f\"number of duplicates in baby_name_only_df = {duplicates.sum()}\")\n",
    "\n",
    "# Drop these duplicates\n",
    "baby_name_only_df = baby_name_only_df.drop_duplicates()\n",
    "duplicates = baby_name_only_df.duplicated()\n",
    "print(f\"number of duplicates in baby_name_only_df = {duplicates.sum()}\")\n",
    "\n",
    "display(baby_name_only_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the names to keep only the ones available in the baby name dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pd.merge to filter rows based on 'names' column\n",
    "word_name_filtered_df = pd.merge(word_name_df.reset_index(), baby_name_only_df, left_on='char_words', right_on='name', how='inner')\n",
    "\n",
    "word_name_filtered_df.drop(columns=['index', 'name'], inplace=True)\n",
    "word_name_filtered_df.set_index(['wiki_ID', 'char_words', 'gender'], inplace=True)\n",
    "print(\"word_name_filtered_df :\")\n",
    "display(word_name_filtered_df.head())\n",
    "\n",
    "# Verify the indexes are unique\n",
    "print(f\"Is the indexing unique ? {word_name_filtered_df.index.is_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the first movie of the CMU dataset, seems ok\n",
    "word_name_filtered_df.loc[975900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name Lieutenant is still in the filtered baby names dataframe. Let's check if this name is in the baby name dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_search = 'Lieutenant'\n",
    "\n",
    "# Check if the name is present\n",
    "is_name_present = name_to_search in baby_name_only_df['name'].values\n",
    "\n",
    "if is_name_present:\n",
    "    print(f\"{name_to_search} is present in the DataFrame.\")\n",
    "else:\n",
    "    print(f\"{name_to_search} is not present in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this dataframe in `tmp` so that we can access this data in the next scraping part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_name_filtered_df.reset_index().to_csv(os.path.join(tmp_folder, 'name_by_movie_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Adding order to the characters in a movie (Scraping part 2)\n",
    "The website [TMDB claims that](https://www.themoviedb.org/bible/movie/59f3b16d9251414f20000003#59f73ca49251416e7100000e) roles for characters are ordered by importance, namely that major roles are always credited before small parts. \n",
    "\n",
    "Therefore make sure you run [this notebook](./scraping/order_characters.ipynb) before continuing. It should have updated the `name_by_movie_ordered_df.csv` file in the `processed` folder, which we will use for our data analysis journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exporting the remaining dataframes\n",
    "We save the files that we will use for our data analysis journey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(movie_df.head())\n",
    "# Export DataFrame to a CSV file in the processed data folder\n",
    "movie_df.to_csv(os.path.join(processed_folder, 'movie_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(baby_name_with_percentage_df.reset_index().head())\n",
    "# Export DataFrame to a CSV file in the processed data folder\n",
    "baby_name_with_percentage_df.reset_index().to_csv(os.path.join(processed_folder, 'baby_name_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with the basic preprocessing! Now we move onto the regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
